# net layer sizes must be multiple of this (try 32 for Nvidia and 64 for AMD)
WORKGROUP_SIZE=1
# 0 = generate net, 1 = train net, 2 = test net, 3 = run net
ENGINE_MODE=0

# directory used for saving/loading net files
NET_DIR=C:/Models/MemNet/
# name used when saving/loading net files
NET_NAME=testnet

# GENERATION SETTINGS

# number of neurons in each layer, starting with input layer, comma separated
NET_SHAPE=784,512,256,128,64,10
# type of neural network to generate (DEFAULT, TEXT_GEN)
NET_TYPE=DEFAULT
# minimum value of randomly generated weights
MIN_WEIGHT=-0.1
# maximum value of randomly generated weights
MAX_WEIGHT=0.1
# activation function used by input layer (LINEAR, LOGISTIC, SWISH, GELU, TANH, GAUSSIAN)
IN_ACT_FUNC=LINEAR
# activation function used by hidden layers (same options as IN_ACT_FUNC)
HID_ACT_FUNC=SWISH
# activation function used by output layer (same options as IN_ACT_FUNC)
OUT_ACT_FUNC=TANH
# equation used to combine neuron input with neuron memory (e.g. a+b, 0 to disable)
MEM_BLEND_FUNC=0

# TRAIN/TEST SETTINGS

# file or folder containing training data
TRAIN_DATA=C:/data/mnist_train.csv
# file or folder containing data for evaluating trained model
TEST_DATA=C:/data/mnist_test.csv
# format of training data (CSV_OUT_IN, CSV_IN_OUT, TEXT_FILES)
DATA_TYPE=CSV_OUT_IN
# specify a function to transform the input data (NONE, TOKENIZE, INDEX_TO_ARRAY, DIV_X where X is number)
INPUT_FUNC=DIV_255
# specify a function to transform the output data (same options as INPUT_FUNC)
OUTPUT_FUNC=INDEX_TO_ARRAY
# number of times to train on all batches
TRAIN_EPOCHS=2
# maximum number of training examples per batch
EXAMPLES_PB=1000
# minimum number of training examples per batch
MIN_BATCH_SIZE=2
# higher values result in quicker training but can produce lower quality nets
LEARN_RATE=0.01
# adjusts the learning rate for neuron biases
LEARN_RATE_BIAS=0.001
# adjusts the learning rate for neuron memory
LEARN_RATE_MEM=0.005
# weights get updated with the average of gradients over this many training steps (0 or 1 to disable)
TRAIN_STEPS=6
# momentum applied in gradient descent (0 to disable, also disabled if TRAIN_STEPS > 1)
MOMENTUM=0.05
# the loss function used to calculate output error (MAE, MSE, CROSS_ENTROPY)
LOSS_FUNC=CROSS_ENTROPY

# TEXT MODEL SETTINGS

# tokenizer word map file for text models
TOKENIZER=C:/wordmap.bin
# tag used to identify end of text sequence
END_TXT_TAG=[END_TEXT]
# only files larger than this (in bytes) will be split into sequences
SPLIT_SIZE=10000000
# enable faster text splitting, disable to reduce RAM usage (TRUE or FALSE)
FAST_SPLIT=TRUE
